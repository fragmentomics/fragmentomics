{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Batch Analysis with FragMentor\n",
    "\n",
    "This notebook demonstrates how to analyze multiple samples efficiently and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from fragmentomics import analyze_sizes\n",
    "from fragmentomics.io import BamReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Multiple BAM Files\n",
    "\n",
    "### Method 1: Python Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze multiple BAM files\n",
    "def analyze_bam_batch(bam_paths):\n",
    "    \"\"\"Analyze multiple BAM files and return a DataFrame.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for path in bam_paths:\n",
    "        path = Path(path)\n",
    "        print(f\"Processing {path.name}...\")\n",
    "        \n",
    "        # Read sizes from BAM\n",
    "        reader = BamReader(path)\n",
    "        sizes = reader.extract_sizes()\n",
    "        \n",
    "        # Analyze\n",
    "        dist = analyze_sizes(sizes)\n",
    "        \n",
    "        # Collect results\n",
    "        results.append({\n",
    "            \"sample\": path.stem,\n",
    "            \"n_fragments\": dist.n_fragments,\n",
    "            \"mean_size\": dist.mean,\n",
    "            \"median_size\": dist.median,\n",
    "            \"ratio_short\": dist.ratio_short,\n",
    "            \"ratio_mono\": dist.ratio_mono,\n",
    "            \"ratio_di\": dist.ratio_di,\n",
    "            \"peak_mono\": dist.peak_mono,\n",
    "            \"periodicity\": dist.periodicity_10bp,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Demo with synthetic data\n",
    "print(\"For real analysis, use:\")\n",
    "print('df = analyze_bam_batch([\"sample1.bam\", \"sample2.bam\", ...])')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Command Line (Recommended for Large Datasets)\n",
    "\n",
    "```bash\n",
    "# Process multiple files in parallel\n",
    "fragmentomics batch samples/*.bam -o results/ --threads 8\n",
    "\n",
    "# Or analyze with a BED file of regions\n",
    "fragmentomics sizes sample.bam --bed regions.bed -o output/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate batch results for demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 20\n",
    "df = pd.DataFrame({\n",
    "    \"sample\": [f\"sample_{i:02d}\" for i in range(n_samples)],\n",
    "    \"group\": [\"healthy\"] * 10 + [\"cancer\"] * 10,\n",
    "    \"n_fragments\": np.random.randint(50000, 200000, n_samples),\n",
    "    \"mean_size\": np.concatenate([\n",
    "        np.random.normal(165, 5, 10),  # Healthy\n",
    "        np.random.normal(145, 8, 10),  # Cancer\n",
    "    ]),\n",
    "    \"ratio_short\": np.concatenate([\n",
    "        np.random.beta(2, 10, 10),  # Healthy: low\n",
    "        np.random.beta(5, 6, 10),   # Cancer: high\n",
    "    ]),\n",
    "    \"ratio_mono\": np.concatenate([\n",
    "        np.random.beta(8, 3, 10),   # Healthy: high\n",
    "        np.random.beta(5, 5, 10),   # Cancer: moderate\n",
    "    ]),\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by group\n",
    "df.groupby(\"group\").agg({\n",
    "    \"mean_size\": [\"mean\", \"std\"],\n",
    "    \"ratio_short\": [\"mean\", \"std\"],\n",
    "    \"ratio_mono\": [\"mean\", \"std\"],\n",
    "}).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize group differences\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for ax, col, title in zip(\n",
    "    axes,\n",
    "    [\"mean_size\", \"ratio_short\", \"ratio_mono\"],\n",
    "    [\"Mean Fragment Size\", \"Short Fragment Ratio\", \"Mononucleosome Ratio\"]\n",
    "):\n",
    "    for group, color in [(\"healthy\", \"#2ecc71\"), (\"cancer\", \"#e74c3c\")]:\n",
    "        data = df[df[\"group\"] == group][col]\n",
    "        ax.hist(data, alpha=0.7, label=group, color=color, bins=8)\n",
    "    ax.set_xlabel(title)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "# df.to_csv(\"batch_results.csv\", index=False)\n",
    "\n",
    "# Export to Excel with multiple sheets\n",
    "# with pd.ExcelWriter(\"batch_results.xlsx\") as writer:\n",
    "#     df.to_excel(writer, sheet_name=\"All Samples\", index=False)\n",
    "#     df.groupby(\"group\").mean().to_excel(writer, sheet_name=\"Group Means\")\n",
    "\n",
    "print(\"Export options:\")\n",
    "print(\"  df.to_csv('results.csv')\")\n",
    "print(\"  df.to_parquet('results.parquet')\")\n",
    "print(\"  df.to_excel('results.xlsx')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Processing\n",
    "\n",
    "For large cohorts, use multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from fragmentomics import analyze_sizes\n",
    "from fragmentomics.io import BamReader\n",
    "\n",
    "def analyze_single_bam(bam_path):\n",
    "    \"\"\"Analyze a single BAM file (for parallel processing).\"\"\"\n",
    "    reader = BamReader(bam_path)\n",
    "    sizes = reader.extract_sizes()\n",
    "    dist = analyze_sizes(sizes)\n",
    "    return dist.to_dict()\n",
    "\n",
    "# Example parallel processing\n",
    "# bam_files = list(Path(\"data/\").glob(\"*.bam\"))\n",
    "# with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "#     results = list(executor.map(analyze_single_bam, bam_files))\n",
    "\n",
    "print(\"For parallel processing:\")\n",
    "print(\"  with ProcessPoolExecutor(max_workers=8) as executor:\")\n",
    "print(\"      results = list(executor.map(analyze_single_bam, bam_files))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regional Analysis\n",
    "\n",
    "Analyze specific genomic regions using a BED file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fragmentomics.io import read_bed, GenomicRegion\n",
    "\n",
    "# Example: Read promoter regions\n",
    "# promoters = read_bed_to_list(\"promoters.bed\")\n",
    "# print(f\"Loaded {len(promoters)} regions\")\n",
    "\n",
    "# Analyze fragments in each region\n",
    "# for region in promoters[:10]:\n",
    "#     reader = BamReader(\"sample.bam\")\n",
    "#     sizes = reader.extract_sizes(region=f\"{region.chrom}:{region.start}-{region.end}\")\n",
    "#     dist = analyze_sizes(sizes)\n",
    "#     print(f\"{region.name}: {dist.n_fragments} fragments, mean={dist.mean:.1f}\")\n",
    "\n",
    "print(\"For regional analysis:\")\n",
    "print(\"  from fragmentomics.io import read_bed_to_list\")\n",
    "print(\"  regions = read_bed_to_list('promoters.bed')\")\n",
    "print(\"  for region in regions:\")\n",
    "print(\"      sizes = reader.extract_sizes(region=region)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Large Datasets\n",
    "\n",
    "1. **Use streaming** â€” FragMentor processes BAMs in chunks, not all in memory\n",
    "2. **Parallel processing** â€” Use `--threads` flag or `ProcessPoolExecutor`\n",
    "3. **Index your BAMs** â€” Ensure `.bai` files exist for fast random access\n",
    "4. **Filter regions** â€” Use `--bed` to focus on regions of interest\n",
    "5. **Save intermediate results** â€” Use JSON output for resumable analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
